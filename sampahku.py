# -*- coding: utf-8 -*-
"""SampahKu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EcxY2BzFKN5hFvA8kA99LgdLlbn2MqWy
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import zipfile

# zip_ref = zipfile.ZipFile('Sesuai Folder Drive', 'r')
zip_ref = zipfile.ZipFile('/content/drive/MyDrive/SampahKu/Dataset/Dataset.zip', 'r')
zip_ref.extractall("tmp/")
zip_ref.close()

# base_dir = 'tmp/Dataset'
base_dir = 'tmp/DATASET'

from pathlib import Path
import imghdr

train_dir = os.path.join(base_dir, 'TRAIN')
test_dir = os.path.join(base_dir, 'TEST')

image_extensions = ['.png', '.jpg']

img_type_accepted = ['jpeg', 'png', 'gif', 'bmp', 'jpg']
for filepath in Path(train_dir).rglob("*"):
  if filepath.suffix.lower() in image_extensions:
    img_type = imghdr.what(filepath)
    if img_type is None:
      print(f'{filepath} is not an image')
    elif img_type not in img_type_accepted:
      print(f'{filepath} is a {img_type}, not accepted by TensorFlow')

image_extensions = ['.png', '.jpg']

img_type_accepted = ['jpeg', 'png', 'gif', 'bmp', 'jpg']
for filepath in Path(test_dir).rglob("*"):
  if filepath.suffix.lower() in image_extensions:
    img_type = imghdr.what(filepath)
    if img_type is None:
      print(f'{filepath} is not an image')
    elif img_type not in img_type_accepted:
      print(f'{filepath} is a {img_type}, not accepted by TensorFlow')

o_train_dir = os.path.join(train_dir, 'O')
r_train_dir = os.path.join(train_dir, 'R')

o_test_dir = os.path.join(test_dir, 'O')
r_test_dir = os.path.join(test_dir, 'R')

print('total training Organik images:', len(os.listdir(o_train_dir)))
print('total training Anorganik images:', len(os.listdir(r_train_dir)))

print('total testing Organik images:', len(os.listdir(o_test_dir)))
print('total testing Anorganik images:', len(os.listdir(r_test_dir)))

o_train_files = os.listdir(o_train_dir)
print(o_train_files[:10])

r_train_files = os.listdir(r_train_dir)
print(r_train_files[:10])

o_test_files = os.listdir(o_test_dir)
print(o_test_files[:10])

r_test_files = os.listdir(r_test_dir)
print(r_test_files[:10])

# prompt: buatkan x.train, x.test, y.train, y.test

import numpy as np
from sklearn.model_selection import train_test_split

# Assuming you have lists of filenames for each category (as shown in your code)
# o_train_files, r_train_files, o_test_files, r_test_files

# Create labels for organic and recyclable waste
o_train_labels = np.zeros(len(o_train_files))  # Organic (label 0)
r_train_labels = np.ones(len(r_train_files))   # Recyclable (label 1)
o_test_labels = np.zeros(len(o_test_files))    # Organic (label 0)
r_test_labels = np.ones(len(r_test_files))     # Recyclable (label 1)

# Combine file lists and labels for train and test datasets
x_train = np.concatenate([o_train_files, r_train_files])
y_train = np.concatenate([o_train_labels, r_train_labels])

x_test = np.concatenate([o_test_files, r_test_files])
y_test = np.concatenate([o_test_labels, r_test_labels])


# Now x_train, y_train, x_test, y_test are ready for use

# Example usage (optional): Split further for validation
# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

print("x_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)
print("x_test shape:", x_test.shape)
print("y_test shape:", y_test.shape)

NUM_CLASSES = 2
Sampahku_classes = ["O", "R"]

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random

image_dir = 'tmp/DATASET/TRAIN/'  # Replace with the correct directory


def display_images(x_train, y_train, num_images=16):
    cols = 4
    rows = 4
    fig = plt.figure(figsize=(2 * cols - 1, 2.5 * rows - 1))

    random_indices = random.sample(range(len(x_train)), num_images)  # Select random images

    for i in range(rows):
        for j in range(cols):
            index = random_indices[i*cols + j]
            filename = x_train[index]
            label = Sampahku_classes[int(y_train[index])]

            # Construct the full image path
            img_path = os.path.join(image_dir, label, filename)

            try:
              img = mpimg.imread(img_path)
              ax = fig.add_subplot(rows, cols, i * cols + j + 1)
              ax.grid('off')
              ax.axis('off')
              ax.imshow(img)
              ax.set_title(label)
            except FileNotFoundError:
              print(f"Image file not found: {img_path}")
              ax = fig.add_subplot(rows, cols, i * cols + j + 1)
              ax.grid('off')
              ax.axis('off')
              ax.text(0.5, 0.5, "Image Not Found", ha='center', va='center')

    plt.show()


display_images(x_train, y_train)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout,LeakyReLU

def make_model():
    model = Sequential()
    model.add(Conv2D(16, (3, 3), input_shape=(32, 32, 3), padding='same'))
    model.add(LeakyReLU(0.1))
    model.add(Conv2D(32, (3, 3), padding='same'))
    model.add(LeakyReLU(0.1))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Conv2D(32, (3, 3), padding='same'))
    model.add(LeakyReLU(0.1))
    model.add(Conv2D(64, (3, 3), padding='same'))
    model.add(LeakyReLU(0.1))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(256))
    model.add(LeakyReLU(0.1))
    model.add(Dropout(0.5))
    model.add(Dense(10, activation='softmax'))

    return model

from keras.optimizers import Adam
from keras import metrics

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)

BATCH_SIZE = 32

from keras.utils import image_dataset_from_directory

train = image_dataset_from_directory(
    directory=train_dir,
    label_mode='categorical',
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    image_size=(224, 224),
    shuffle=True,
    seed=42,
)

val = image_dataset_from_directory(
    directory=test_dir,
    label_mode='categorical',
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    image_size=(224, 224),
    shuffle=False,
)

history = model.fit(
    train,
    validation_data=val,
    epochs=10,
    steps_per_epoch=50,
    validation_steps=5,
    verbose=2,
)

predictions = model.predict(val)

import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

y_pred = np.argmax(predictions, axis=1)
y_true = np.concatenate([y for x, y in val], axis=0)
y_true = np.argmax(y_true, axis=1)

y_dict = {
    0: 'O',
    1: 'R',
}
y_pred = np.vectorize(y_dict.get)(y_pred)
y_true =np.vectorize(y_dict.get)(y_true)

cm = confusion_matrix(y_true, y_pred)

print(classification_report(y_true, y_pred))

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_acc = range(len(acc))
epochs_loss = range(len(loss))

fig, (acc_fig, loss_fig) = plt.subplots(1, 2, figsize=(17, 5))

acc_fig.plot(epochs_acc, acc, '#800000', label='Training accuracy')
acc_fig.plot(epochs_acc, val_acc, '#000080', label='Validation accuracy')
acc_fig.set_title('Training and validation accuracy')
acc_fig.legend(["Training", "Validation"], loc ="lower right")

loss_fig.plot(epochs_loss, loss, '#800000', label='Training loss')
loss_fig.plot(epochs_loss, val_loss, '#000080', label='Validation loss')
loss_fig.set_title('Training and validation loss')
loss_fig.legend(["Training", "Validation"], loc ="upper right")

plt.show()

import numpy as np
from google.colab import files
from keras.utils import load_img, img_to_array

uploaded = files.upload()

for fn in uploaded.keys():

  path = fn
  img = load_img(path, target_size=(224, 224))
  x = img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = model.predict(images, batch_size=8)
  print(fn)

  print('O', '{:.5f}'.format(classes[0, 0]))
  print('R', '{:.5f}'.format(classes[0, 1]))

model.save('SampahKu.keras')

from tensorflow import keras

model_path = '/content/SampahKu.keras'
model = keras.models.load_model(model_path)

from tensorflow import lite

converter = lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with open('SampahKu_TFLiteModel.tflite', 'wb') as f:
  f.write(tflite_model)

label = ['O', 'R']

with open('label.txt', 'w') as f:
  f.write('\n'.join(label))

model.save_weights("model_sampahku_tf.weights.h5") # Changed the filename to include the .weights.h5 extension

# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense,
#                                      Activation, Dropout, LeakyReLU, RandomFlip, Rescaling)
# from tensorflow.keras.applications import MobileNet

# def make_model():
#     # Menggunakan MobileNet sebagai base_model
#     base_model = MobileNet(
#         weights='imagenet',
#         include_top=False,
#         input_shape=(224, 224, 3),
#     )
#     base_model.trainable = False  # Freeze base_model agar tidak ikut dilatih

#     # Membuat model Sequential
#     model = Sequential([
#         # Preprocessing Layer
#         Rescaling(1.0 / 255.0, input_shape=(224, 224, 3)),  # Normalisasi data
#         RandomFlip(mode='horizontal'),  # Augmentasi data (flip horizontal)

#         # Base Model
#         base_model,

#         # Fully Connected Layer
#         Flatten(),
#         Dense(32, activation='relu'),
#         Dropout(0.1),
#         Dense(2, activation='sigmoid'),
#     ])

#     return model